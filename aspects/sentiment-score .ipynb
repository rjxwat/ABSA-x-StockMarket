{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9564929,"sourceType":"datasetVersion","datasetId":5829317},{"sourceId":9565636,"sourceType":"datasetVersion","datasetId":5829872}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The purpose of this code is to get overall sentiment score of the daily news so as to compare with aspect sentiment score","metadata":{}},{"cell_type":"markdown","source":"Its best to run the code in KAGGLE. THE LINK TO THE CODE IS https://www.kaggle.com/code/adi253/sentiment-score","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-07T08:07:15.884105Z","iopub.execute_input":"2024-10-07T08:07:15.884777Z","iopub.status.idle":"2024-10-07T08:07:17.090514Z","shell.execute_reply.started":"2024-10-07T08:07:15.884741Z","shell.execute_reply":"2024-10-07T08:07:17.089562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/newdata/TATA_grouped.csv\")  # can change to other csvs ","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:36:14.858895Z","iopub.execute_input":"2024-10-07T08:36:14.859275Z","iopub.status.idle":"2024-10-07T08:36:15.040097Z","shell.execute_reply.started":"2024-10-07T08:36:14.859239Z","shell.execute_reply":"2024-10-07T08:36:15.039027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n\ndf.dropna(inplace=True) # dropping null values     \n\ndf.reset_index(drop=True, inplace=True)# when u drop the row index would be still there ","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:36:15.581099Z","iopub.execute_input":"2024-10-07T08:36:15.581470Z","iopub.status.idle":"2024-10-07T08:36:15.592796Z","shell.execute_reply.started":"2024-10-07T08:36:15.581432Z","shell.execute_reply":"2024-10-07T08:36:15.591825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:36:18.121725Z","iopub.execute_input":"2024-10-07T08:36:18.122061Z","iopub.status.idle":"2024-10-07T08:36:18.127949Z","shell.execute_reply.started":"2024-10-07T08:36:18.122030Z","shell.execute_reply":"2024-10-07T08:36:18.126976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"news\"][375]","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:36:20.612160Z","iopub.execute_input":"2024-10-07T08:36:20.612929Z","iopub.status.idle":"2024-10-07T08:36:20.618572Z","shell.execute_reply.started":"2024-10-07T08:36:20.612882Z","shell.execute_reply":"2024-10-07T08:36:20.617648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:07:24.538599Z","iopub.execute_input":"2024-10-07T08:07:24.539439Z","iopub.status.idle":"2024-10-07T08:07:25.748057Z","shell.execute_reply.started":"2024-10-07T08:07:24.539401Z","shell.execute_reply":"2024-10-07T08:07:25.747248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:07:25.902942Z","iopub.execute_input":"2024-10-07T08:07:25.903324Z","iopub.status.idle":"2024-10-07T08:07:31.393295Z","shell.execute_reply.started":"2024-10-07T08:07:25.903261Z","shell.execute_reply":"2024-10-07T08:07:31.392335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\") # load the tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\") # load the model ","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:07:31.394916Z","iopub.execute_input":"2024-10-07T08:07:31.395363Z","iopub.status.idle":"2024-10-07T08:08:06.800789Z","shell.execute_reply.started":"2024-10-07T08:07:31.395328Z","shell.execute_reply":"2024-10-07T08:08:06.799975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating two columns for predicted sentiment(either positive negative or neutral) and Sentiment score/probability","metadata":{}},{"cell_type":"code","source":"df['Predicted Sentiment'] = None   \ndf['Sentiment Probability'] = None","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:36:25.448732Z","iopub.execute_input":"2024-10-07T08:36:25.449422Z","iopub.status.idle":"2024-10-07T08:36:25.456569Z","shell.execute_reply.started":"2024-10-07T08:36:25.449380Z","shell.execute_reply":"2024-10-07T08:36:25.455660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The purpose of the below block is to run the sequence model for classification which is trained on finance dataset.","metadata":{}},{"cell_type":"code","source":"import torch  # Import PyTorch for inference\nimport scipy  # Import SciPy for softmax function\n\n# Keyword arguments for tokenizing the input text:\n# - padding=True: Pads the input to the maximum length if necessary\n# - truncation=True: Truncates the input if it exceeds the maximum length\n# - max_length=512: Ensures that no input sequence exceeds 512 tokens\ntokenizer_kwargs = {\"padding\": True, \"truncation\": True, \"max_length\": 512}\n\n# Loop over each row in the DataFrame `df`\nfor i in range(len(df)):\n    print(i)  # Print the current row index for tracking progress\n    x = df.loc[i, \"news\"]  # Extract the \"news\" column value (text) for the current row\n    \n    # Perform inference without calculating gradients (saves memory and speeds up execution)\n    with torch.no_grad():\n        # Tokenize the input text using the tokenizer with the specified arguments\n        input_sequence = tokenizer(x, return_tensors=\"pt\", **tokenizer_kwargs)\n        \n        # Pass the tokenized input through the model to get the output\n        output = model(**input_sequence)\n        \n        # Extract the logits (raw output values) from the model\n        logits = model(**input_sequence).logits\n        \n        # Compute softmax probabilities for each label using SciPy's softmax function\n        # `model.config.id2label` maps label IDs to label names\n        scores = {\n            k: v\n            for k, v in zip(\n                model.config.id2label.values(),  # Convert label IDs to label names\n                scipy.special.softmax(logits.numpy().squeeze()),  # Apply softmax to logits and squeeze the tensor\n            )\n        }\n    \n    # Get the label with the highest score (predicted sentiment)\n    sentiment = max(scores, key=scores.get)\n    \n    # Get the highest probability associated with the predicted sentiment\n    probability = max(scores.values())\n    \n    # Store the predicted sentiment in the DataFrame under the \"Predicted Sentiment\" column\n    df.loc[i, \"Predicted Sentiment\"] = sentiment\n    \n    # Store the probability of the predicted sentiment in the \"Sentiment Probability\" column\n    df.loc[i, \"Sentiment Probability\"] = probability\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:14:13.610863Z","iopub.execute_input":"2024-10-07T08:14:13.611646Z","iopub.status.idle":"2024-10-07T08:14:13.615828Z","shell.execute_reply.started":"2024-10-07T08:14:13.611578Z","shell.execute_reply":"2024-10-07T08:14:13.614763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:53:39.497724Z","iopub.execute_input":"2024-10-07T08:53:39.498269Z","iopub.status.idle":"2024-10-07T08:53:39.511878Z","shell.execute_reply.started":"2024-10-07T08:53:39.498222Z","shell.execute_reply":"2024-10-07T08:53:39.510882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Predicted Sentiment\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:53:39.513383Z","iopub.execute_input":"2024-10-07T08:53:39.513762Z","iopub.status.idle":"2024-10-07T08:53:39.528353Z","shell.execute_reply.started":"2024-10-07T08:53:39.513717Z","shell.execute_reply":"2024-10-07T08:53:39.527488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/tatasentiment.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:53:39.530441Z","iopub.execute_input":"2024-10-07T08:53:39.530804Z","iopub.status.idle":"2024-10-07T08:53:39.802123Z","shell.execute_reply.started":"2024-10-07T08:53:39.530769Z","shell.execute_reply":"2024-10-07T08:53:39.801145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
